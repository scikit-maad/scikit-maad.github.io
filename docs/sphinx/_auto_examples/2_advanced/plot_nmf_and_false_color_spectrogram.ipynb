{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\nSignal decomposition and false-color spectrograms\n===============================================\n\nSoundscapes result from a combination of multiple signals that are mixed-down\ninto a single time-series. Unmixing these signals can be regarded as an \nimportant preprocessing step for further analyses of individual components.\nHere, we will combine the robust characterization capabilities of \nthe bidimensional wavelets [1] with an advanced signal decomposition tool, the \nnon-negative-matrix factorization (NMF)[2]. NMF is a widely used tool to analyse\nhigh-dimensional data that automatically extracts sparse and meaningfull components\nof non-negative matrices. Audio spectrograms are in essence sparse and \nnon-negative matrices, and hence well suited to be decomposed with NMF. This \ndecomposition can be further used to generate false-color spectrograms to \nrapidly identify patterns in soundscapes and increase the interpretability of \nthe signal [3]. This example shows how to use the scikit-maad package to easily \ndecompose audio signals and visualize false-colour spectrograms.\n\n**Dependencies**: This example requires the Python package scikit-learn v0.24 or greater.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# sphinx_gallery_thumbnail_path = '../_images/sphx_glr_plot_nmf_and_false_color_spectrogram_003.png'\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom maad import sound, features\nfrom maad.util import power2dB, plot2d\nfrom skimage import transform\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.decomposition import NMF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load audio from disk\n--------------------\nLoad the audio file and compute the spectrogram.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "s, fs = sound.load('../../data/spinetail.wav')\nSxx, tn, fn, ext = sound.spectrogram(s, fs, nperseg=1024, noverlap=512)\n\nSxx_db = power2dB(Sxx, db_range=70)\nSxx_db = transform.rescale(Sxx_db, 0.5, anti_aliasing=True, multichannel=False)  # rescale for faster computation\nplot2d(Sxx_db, figsize=(4,10), extent=ext)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Filter the spectrogram with 2D wavelets\n---------------------------------------\nCompute feature with ``shape_features_raw`` to get the raw output of the \nspectrogram filtered by the filterbank composed of 2D Gabor wavelets. This\nraw output can be fed to the NMF algorithm to decompose the spectrogram into\nelementary basis spectrograms.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "shape_im, params = features.shape_features_raw(Sxx_db, resolution='low')\n\n# Format the output as an array for decomposition\nX = np.array(shape_im).reshape([len(shape_im), Sxx_db.size]).transpose()\n\n# Decompose signal using non-negative matrix factorization\nY = NMF(n_components=3, init='random', random_state=0).fit_transform(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Arrange into RGBA color model\n-----------------------------\nNormalize the data and combine the three NMF basis spectrograms and the\nintensity spectrogram into a single array to fit the RGBA color model. RGBA\nstands for Red, Green, Blue and Alpha, where alpha indicates how opaque each\npixel is.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "Y = MinMaxScaler(feature_range=(0,1)).fit_transform(Y)\nintensity = 1 - (Sxx_db - Sxx_db.min()) / (Sxx_db.max() - Sxx_db.min())\nplt_data = Y.reshape([Sxx_db.shape[0], Sxx_db.shape[1], 3])\nplt_data = np.dstack((plt_data, intensity))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Visualize output\n----------------\nFinally, plot the resulting basis spectrogram as separate elements and \ncombine them to produce a false-colour spectrogram using the RGBA color \nmodel.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(3,1, figsize=(10,8))\nfor idx, ax in enumerate(axes):\n    ax.imshow(plt_data[:,:,idx], origin='lower', aspect='auto', \n              interpolation='bilinear')\n    ax.set_axis_off()\n    ax.set_title('Basis ' + str(idx+1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The first basis spectrogram shows fine and rapid modulations of the signal.\nBoth signals have these features and hence both are delineated in this\nbasis. The second basis highlights the short calls on the background, and the \nthird component highlights the longer vocalizations of the spinetail. \nThe three components can be mixed up to compose a false-colour spectrogram\nwhere it can be easily distinguished the different sound sources by color.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(2,1, figsize=(10,6))\nax[0].imshow(Sxx_db, origin='lower', aspect='auto', interpolation='bilinear', cmap='gray')\nax[0].set_axis_off()\nax[0].set_title('Spectrogram')\nax[1].imshow(plt_data, origin='lower', aspect='auto', interpolation='bilinear')\nax[1].set_axis_off()\nax[1].set_title('False-color spectrogram')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "References\n-----------\n1. Sifre, L., & Mallat, S. (2013). Rotation, scaling and deformation invariant scattering for texture discrimination. Computer Vision and Pattern Recognition (CVPR), 2013 IEEE Conference On, 1233\u20131240. http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6619007\n2. Lee, D., & Sueng, S. (1999). Learning the parts of objects by non-negative matrix factorization. Nature, 401, 788\u2013791. https://doi.org/10.1038/44565\n3. Towsey, M., Znidersic, E., Broken-Brow, J., Indraswari, K., Watson, D. M., Phillips, Y., Truskinger, A., & Roe, P. (2018). Long-duration, false-colour spectrograms for detecting species in large audio data-sets. Journal of Ecoacoustics, 2(1), 1\u20131. https://doi.org/10.22261/JEA.IUSWUI\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}