{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\nFind Regions of interest (ROIs) in a spectrogram\n================================================\n\nA spectrogram is a time-frequency (2d) representation of a audio recording. \nEach acoustic event nested in the audio recording is represented by an acoustic\nsignature. When sounds does not overlap in time and frequency, it is possible\nto extract automatically the acoustic signature as a region of interest (ROI) \nby different image processing tools such as binarization, double thresholding,\nmathematical morphology tools...\n\nDependencies: To execute this example you will need to have installed the \nscikit-image, scikit-learn and pandas Python packages.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# sphinx_gallery_thumbnail_path = '../_images/sphx_glr_compare_auto_and_manual_rois_selection.png'\n\nimport numpy as np\nimport pandas as pd\nfrom maad import sound, rois, features\nfrom maad.util import (power2dB, plot2d, format_features, read_audacity_annot, \n                       overlay_rois, overlay_centroid)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First, load and audio file and compute the power spectrogram.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "s, fs = sound.load('../../data/cold_forest_daylight.wav')\n\ndB_max = 96\n\nSxx_power, tn, fn, ext = sound.spectrogram(s, fs, nperseg=1024, noverlap=1024//2)\n\n# Convert the power spectrogram into dB, add dB_max which is the maximum decibel\n# range when quantification bit is 16bits and display the result\nSxx_db = power2dB(Sxx_power) + dB_max\nplot2d(Sxx_db, **{'vmin':0, 'vmax':dB_max, 'extent':ext})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then, relevant acoustic events are extracted directly from the power \nspectrogram based on a double thresholding technique. The result is binary\nimage called a mask. Double thresholding technique is more sophisticated than\nbasic thresholding based on a single value. First, a threshold selects pixels\nwith high value (i.e. high acoustic energy). They should belong to an acoustic\nevent. They are called seeds. From these seeds, we aggregate pixels connected\nto the seed with value higher than the second threslhold. These new pixels \nbecome seed and the aggregating process continue until no more new pixels are\naggregated, meaning that there is no more connected pixels with value upper \nthan the second threshold value.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# First we remove the stationary background in order to increase the contrast [1]\n# Then we convert the spectrogram into dB\nSxx_power_noNoise= sound.median_equalizer(Sxx_power, display=True, **{'extent':ext})\nSxx_db_noNoise = power2dB(Sxx_power_noNoise)\n\n# Then we smooth the spectrogram in order to facilitate the creation of masks as\n# small sparse details are merged if they are close to each other\nSxx_db_noNoise_smooth = sound.smooth(Sxx_db_noNoise, std=0.5, \n                         display=True, savefig=None, \n                         **{'vmin':0, 'vmax':dB_max, 'extent':ext})\n\n# Then we create a mask (i.e. binarization of the spectrogram) by using the \n# double thresholding technique\nim_mask = rois.create_mask(im=Sxx_db_noNoise_smooth, mode_bin ='relative', \n                           bin_std=8, bin_per=0.5,\n                           verbose=False, display=False)\n\n# Finaly, we put together pixels that belong to the same acoustic event, and \n# remove very small events (<=25 pixel\u00b2)\nim_rois, df_rois = rois.select_rois(im_mask, min_roi=25, max_roi=None, \n                                 display= True,\n                                 **{'extent':ext})\n    \n# format dataframe df_rois in order to convert pixels into time and frequency\ndf_rois = format_features(df_rois, tn, fn)\n\n# overlay bounding box on the original spectrogram\nax0, fig0 = overlay_rois(Sxx_db, df_rois, **{'vmin':0, 'vmax':dB_max, 'extent':ext})\n\n# Compute and visualize centroids\ndf_centroid = features.centroid_features(Sxx_db, df_rois, im_rois)\ndf_centroid = format_features(df_centroid, tn, fn)\nax0, fig0 = overlay_centroid(Sxx_db, df_centroid, savefig=None,\n                             **{'vmin':0,'vmax':dB_max,'extent':ext,'ms':4, \n                                'marker':'+', 'fig':fig0, 'ax':ax0})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's compare with the manual annotation (Ground Truth GT) obtained with \nAudacity software.\nEach acoustic signature is manually selected and labeled. All similar acoustic \nsignatures are labeled with the same name\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "df_rois_GT = read_audacity_annot('../../data/cold_forest_daylight_label.txt')  ## annotations using Audacity\n\n# drop rows with frequency and time outside of tn and fn\ndf_rois_GT = df_rois_GT[(df_rois_GT.min_t >= tn.min()) & \n                        (df_rois_GT.max_t <= tn.max()) & \n                        (df_rois_GT.min_f >= fn.min()) & \n                        (df_rois_GT.max_f <= fn.max())]\n\n# format dataframe df_rois in order to convert time and frequency into pixels\ndf_rois_GT = format_features(df_rois_GT, tn, fn)\n\n# overlay bounding box on the original spectrogram\nax1, fig1 = overlay_rois(Sxx_db, df_rois_GT, **{'vmin':0,'vmax':dB_max,'extent':ext})\n    \n# Compute and visualize centroids\ndf_centroid_GT = features.centroid_features(Sxx_db, df_rois_GT)\ndf_centroid_GT = format_features(df_centroid_GT, tn, fn)\nax1, fig1 = overlay_centroid(Sxx_db, df_centroid_GT, savefig=None, \n                             **{'vmin':0,'vmax':dB_max,'extent':ext,\n                                'ms':2, 'marker':'+','color':'blue',\n                                'fig':fig1, 'ax':ax1})\n\n# print informations about the rois\nprint ('Total number of ROIs : %2.0f' %len(df_rois_GT))\nprint ('Number of different ROIs : %2.0f' %len(np.unique(df_rois_GT['label'])))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we cluster the ROIS depending on 3 ROIS features :\n- centroid_f : frequency position of the roi centroid \n- duration_t : duration of the roi\n- bandwidth_f : frequency bandwidth of the roi\nThe clustering is done by the so-called KMeans clustering algorithm.\nThe number of attended clustering is the number of clusters found with \nmanual annotation.\nFinally, each rois is labeled with the corresponding cluster number predicted\nby KMeans\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler\n\n# select features to perform KMeans clustering\nFEATURES = ['centroid_f','duration_t','bandwidth_f','area_tf']\n\n# Prepare the features in order to have zero mean and same variance\nX = StandardScaler().fit_transform(df_centroid[FEATURES])\n\n# perform KMeans with the same number of clusters as with the manual annotation  \nNN_CLUSTERS = len(np.unique(df_rois_GT['label'])) \nlabels = KMeans(n_clusters=NN_CLUSTERS, random_state=0).fit_predict(X)\n\n# Replace the unknow label by the cluster number predicted by KMeans\ndf_centroid['label'] = [str(i) for i in labels] \n\n# overlay color bounding box corresponding to the label, and centroids\n# on the original spectrogram\nax2, fig2 = overlay_rois(Sxx_db, df_centroid, **{'vmin':0,'vmax':dB_max,'extent':ext})\nax2, fig2 = overlay_centroid(Sxx_db, df_centroid, savefig=None, \n                             **{'vmin':0,'vmax':dB_max,'extent':ext,'ms':2, \n                                'fig':fig2, 'ax':ax2})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It is possible to extract Rois directly from the audio waveform without \ncomputing the spectrogram. This works well if there is no big overlap between\neach acoustic signature and you \nFirst, we have to define the frequency bandwidth where to find acoustic events\nIn our example, there are clearly 3 frequency bandwidths (low : l, medium:m\nand high : h). \nWe know that we have mostly short (ie. s) acoustic events in low, med and high\nfrequency bandwidths but also a long (ie l) acoustic events in med.\nTo extract \n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "df_rois_sh = rois.find_rois_cwt(s, fs, flims=[7000, 8000], tlen=0.2, th=0.000001)\ndf_rois_sm = rois.find_rois_cwt(s, fs, flims=[3500, 5500], tlen=0.2, th=0.000001)\ndf_rois_lm = rois.find_rois_cwt(s, fs, flims=[2000, 7500], tlen=2,   th=0.0001)\ndf_rois_sl = rois.find_rois_cwt(s, fs, flims=[1800, 3000], tlen=0.2, th=0.000001)\n\n## concat df\ndf_rois_WAV =pd.concat([df_rois_sh, df_rois_sm, df_rois_lm, df_rois_sl], ignore_index=True)\n\n# drop rows with frequency and time outside of tn and fn\ndf_rois_WAV = df_rois_WAV[(df_rois_WAV.min_t >= tn.min()) & \n                                      (df_rois_WAV.max_t <= tn.max()) & \n                                      (df_rois_WAV.min_f >= fn.min()) & \n                                      (df_rois_WAV.max_f <= fn.max())]\n    \n# get features: centroid, \ndf_rois_WAV = format_features(df_rois_WAV, tn, fn)\ndf_centroid_WAV = features.centroid_features(Sxx_db, df_rois_WAV)\n\nax3, fig3 = overlay_rois(Sxx_db, df_rois_WAV, **{'vmin':0,'vmax':dB_max,\n                                                      'extent':ext})\ndf_centroid_WAV = format_features(df_centroid_WAV, tn, fn)\nax3, fig3 = overlay_centroid(Sxx_db, df_centroid_WAV, savefig=None, \n                             **{'vmin':0,'vmax':dB_max,'extent':ext,\n                                'ms':2, 'fig':fig3, 'ax':ax3})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Prepare the features in order to have zero mean and same variance\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "X = StandardScaler().fit_transform(df_centroid_WAV[FEATURES])\n\n# perform KMeans with the same number of clusters as with the manual annotation  \nlabels = KMeans(n_clusters=NN_CLUSTERS, random_state=0).fit_predict(X)\n\n# Replace the unknow label by the cluster number predicted by KMeans\ndf_centroid_WAV['label'] = [str(i) for i in labels] \n\n# overlay color bounding box corresponding to the label, and centroids\n# on the original spectrogram\nax4, fig4 = overlay_rois(Sxx_db, df_centroid_WAV, **{'vmin':0,'vmax':dB_max,\n                                                          'extent':ext})\nax4, fig4 = overlay_centroid(Sxx_db, df_centroid_WAV, savefig=None, \n                             **{'vmin':0,'vmax':dB_max,'extent':ext,\n                                'ms':2,'fig':fig4, 'ax':ax4})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "References\n-----------\n1.Towsey, M., 2013b. Noise Removal from Wave-forms and Spectrograms Derived from\n  Natural Recordings of the Environment. Queensland University of Technology,\n  Brisbane\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}