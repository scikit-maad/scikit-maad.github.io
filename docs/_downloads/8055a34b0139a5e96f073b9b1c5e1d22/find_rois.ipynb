{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\nFind Regions of interest (ROIs) in a spectrogram\n================================================\n\nA spectrogram is a time-frequency (2d) representation of a audio recording. \nEach acoustic event nested in the audio recording is represented by an acoustic\nsignature. When sounds does not overlap in time and frequency, it is possible\nto extract automatically the acoustic signature as a region of interest (ROI) \nby different image processing tools such as binarization, double thresholding,\nmathematical morphology tools...\n\nDependencies: To execute this example you will need to have installed the \nscikit-image and pandas Python packages.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# sphinx_gallery_thumbnail_path = '../_images/sphx_glr_find_rois.png'\n\nfrom maad import sound, rois, features\nfrom maad.util import power2dB, plot2D, format_features, read_audacity_annot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First, load and audio file and compute the power spectrogram.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "s, fs = sound.load('../data/cold_forest_daylight.wav')\n\nt0 = 0\nt1 = 20\nf0 = 100\nf1 = 10000\ndB_max = 96\n\nSxx, tn, fn, ext = sound.spectrogram(s, fs, nperseg=1024, noverlap=512, \n                                     fcrop=(f0,f1), tcrop=(t0,t1))\n\n# Convert the power spectrogram into dB, add dB_max which is the maximum decibel\n# range when quantification bit is 16bits and display the result\nSxx_db = power2dB(Sxx) + dB_max\nplot2D(Sxx_db, **{'vmin':0, 'vmax':dB_max, 'extent':ext})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then, relevant acoustic events are extracted directly from the power \nspectrogram based on a double thresholding technique. The result is binary\nimage called a mask. Double thresholding technique is more sophisticated than\nbasic thresholding based on a single value. First, a threshold selects pixels\nwith high value (i.e. high acoustic energy). They should belong to an acoustic\nevent. They are called seeds. From these seeds, we aggregate pixels connected\nto the seed with value higher than the second threslhold. These new pixels \nbecome seed and the aggregating process continue until no more new pixels are\naggregated, meaning that there is no more connected pixels with value upper \nthan the second threshold value.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# First we remove the stationary background in order to increase the contrast\nSxx_db_noNoise, _ = rois.remove_background_along_axis(Sxx_db, mode='ale', \n                                                 display=True, \n                                                 ext=ext)\n\n# Then we smooth the spectrogram in order to facilitate the creation of masks as\n# small sparse details are merged if they are close to each other\nSxx_db_noNoise_smooth = rois.smooth(Sxx_db_noNoise, ext, std=1, \n                         display=True, savefig=None)\n\n# Then we create a mask (i.e. binarization of the spectrogram) by using the \n# double thresholding technique\nim_mask = rois.create_mask(im=Sxx_db_noNoise_smooth, ext=ext, \n                           mode_bin ='relative', bin_std=6, bin_per=0.5,\n                           verbose=False, display=False)\n\n# Finaly, we put together pixels that belong to the same acoustic event, and \n# remove very small events (<=25 pixel\u00b2)\nim_rois, df_rois = rois.select_rois(im_mask, min_roi=25, max_roi=None, \n                                 ext=ext, display= False,\n                                 figsize=(4,(t1-t0)))\n    \n# format dataframe df_rois in order to convert pixels into time and frequency\ndf_rois = format_features(df_rois, tn, fn)\n\n# overlay bounding box on the original spectrogram\nax, fig = rois.overlay_rois(Sxx_db, ext, df_rois, vmin=0, vmax=96)\n\n# Compute and visualize centroids\ndf_centroid = features.centroid_features(Sxx_db, df_rois, im_rois)\ndf_centroid = format_features(df_centroid, tn, fn)\nax, fig = features.overlay_centroid(Sxx_db, ext, df_centroid, savefig=None,\n                                    vmin=0, vmax=96, marker='+',ms=2,\n                                    fig=fig, ax=ax)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, we can compare with manual annotation performed with Audacity software.\nEach acoustic signature is manually selected and labeled. All similar acoustic \nsignatures are labeled with the same name\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "df_rois = read_audacity_annot('../data/cold_forest_daylight_label.txt')  ## annotations using Audacity\n\n# format dataframe df_rois in order to convert time and frequency into pixels\ndf_rois = format_features(df_rois, tn, fn)\n\n# overlay bounding box on the original spectrogram\nax, fig = rois.overlay_rois(Sxx_db, ext, df_rois, vmin=0, vmax=96)\n    \n# Compute and visualize centroids\ndf_centroid = features.centroid_features(Sxx_db, df_rois)\ndf_centroid = format_features(df_centroid, tn, fn)\nax, fig = features.overlay_centroid(Sxx, ext, df_centroid, savefig=None, \n                                    vmin=-0, vmax=96, ms=2, color='blue',\n                                    fig=fig, ax=ax)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "References\n-----------\n1. Sifre, L., & Mallat, S. (2013). Rotation, scaling and deformation invariant scattering for texture discrimination. Computer Vision and Pattern Recognition (CVPR), 2013 IEEE Conference On, 1233\u20131240. http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6619007\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}