{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\nUSE MULTICPU FUNCTIONALITY TO COMPUTE INDICES\n==============================================\n\nAcoustic indices can summarize aspects of the acoustic energy distribution in\naudio recordings and are widely used to characterize animal acoustic communities[1-3].\nIn this example, we will see how to eficiently compute multiple acoustic indices, \nand present basics post-processing posibilities. The audio recordings used in this \nexample can be downloaded from the open GitHub repository \n(https://github.com/scikit-maad/scikit-maad/tree/production/data).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# sphinx_gallery_thumbnail_path = './_images/sphx_glr_plot_extract_alpha_indices_multicpu_001.png'\n\nimport pandas as pd\nimport os\nimport time\nimport matplotlib.pyplot as plt\n\n# Parallel processing packages\n# from functools import partial\nfrom tqdm import tqdm\nfrom concurrent import futures\n\nfrom maad import sound, features\nfrom maad.util import date_parser"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Set Variables\n-------------\nWe list all spectral and temporal acoustic indices that will be computed.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "SPECTRAL_FEATURES=['MEANf','VARf','SKEWf','KURTf','NBPEAKS','LEQf', \n'ENRf','BGNf','SNRf','Hf', 'EAS','ECU','ECV','EPS','EPS_KURT','EPS_SKEW','ACI',\n'NDSI','rBA','AnthroEnergy','BioEnergy','BI','ROU','ADI','AEI','LFC','MFC','HFC',\n'ACTspFract','ACTspCount','ACTspMean', 'EVNspFract','EVNspMean','EVNspCount',\n'TFSD','H_Havrda','H_Renyi','H_pairedShannon', 'H_gamma', 'H_GiniSimpson','RAOQ',\n'AGI','ROItotal','ROIcover']\n\nTEMPORAL_FEATURES=['ZCR','MEANt', 'VARt', 'SKEWt', 'KURTt',\n               'LEQt','BGNt', 'SNRt','MED', 'Ht','ACTtFraction', 'ACTtCount', \n               'ACTtMean','EVNtFraction', 'EVNtMean', 'EVNtCount']\n\n# Parameters of the audio recorder. This is not a mandatory but it allows\n# to compute the sound pressure level of the audio file (dB SPL) as a \n# sonometer would do.\nS = -35         # Sensbility microphone-35dBV (SM4) / -18dBV (Audiomoth)   \nG = 26+16       # Amplification gain (26dB (SM4 preamplifier))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We parse the directory were the audio dataset is located in order to get a df with date \nand fullfilename. As the data were collected with a SM4 audio recording device\nwe set the dateformat agument to 'SM4' in order to be able to parse the date\nfrom the filename. In case of Audiomoth, the date is coded as Hex in the \nfilename. The path to the audio dataset is \"../../data/indices/\".\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "df = date_parser(\"../../data/indices/\", dateformat='SM4', verbose=True)\n\n# Date is used as index. Reset the index in order to get back Date as column\ndf.reset_index(inplace = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define the function that will be used for batch processing\n----------------------------------------------------------\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# This function should work by itself without waiting for another process\n# Here we defined a function to process a single audio file. The input arguments\n# are the path to the audio file and the recording date of the audio file. Both\n# arguments are in the dataframe df given by the function date_parser.\n\ndef single_file_processing (audio_path,\n                            date) :\n    \"\"\"\n    Parameters\n    ----------\n    audio_path : string\n        full path to the audio file (.wav) to process.\n        The full path is in the dataframe given by the function date_parser\n    date : datetime\n        date of recording of the audio file. \n        The date is in the dataframe given by the function date_parser\n\n    Returns\n    -------\n    df_indices : dataframe\n        Dataframe containing all the temporal and spectral indices, as well as\n        the audio path ('file' column) and the recording date ('Date' column)\n\n    \"\"\"\n        \n    # Load the original sound (16bits) and get the sampling frequency fs\n    try :\n        wave,fs = sound.load(filename=audio_path, \n                             channel='left', \n                             detrend=True, \n                             verbose=False)\n\n        \"\"\" ===================================================================\n                         Computation in the time domain \n        ====================================================================\"\"\" \n    \n        # compute all the audio indices and store them into a DataFrame\n        # dB_threshold and rejectDuration are used to select audio events.\n        df_audio_ind = features.all_temporal_alpha_indices(\n                                    wave, fs, \n                                    gain = G, sensibility = S,\n                                    dB_threshold = 3, rejectDuration = 0.01,\n                                    verbose = False, display = False)\n        \n        \"\"\" ===================================================================\n                         Computation in the frequency domain \n        ====================================================================\"\"\"\n     \n        # Compute the Power Spectrogram Density (PSD) : Sxx_power\n        Sxx_power,tn,fn,ext = sound.spectrogram (\n                                        wave, fs, window='hanning', \n                                        nperseg = 1024, noverlap=1024//2, \n                                        verbose = False, display = False, \n                                        savefig = None)   \n        \n        # compute all the spectral indices and store them into a DataFrame \n        # flim_low, flim_mid, flim_hi corresponds to the frequency limits in Hz \n        # that are required to compute somes indices (i.e. NDSI)\n        # if R_compatible is set to 'soundecology', then the output are similar to \n        # soundecology R package.\n        # mask_param1 and mask_param2 are two parameters to find the regions of \n        # interest (ROIs). These parameters need to be adapted to the dataset in \n        # order to select ROIs\n        df_spec_ind, _ = features.all_spectral_alpha_indices(\n                                                Sxx_power,\n                                                tn,fn,\n                                                flim_low = [0,1500], \n                                                flim_mid = [1500,8000], \n                                                flim_hi  = [8000,20000], \n                                                gain = G, sensitivity = S,\n                                                verbose = False, \n                                                R_compatible = 'soundecology',\n                                                mask_param1 = 6, \n                                                mask_param2=0.5,\n                                                display = False)\n        \n        \"\"\" ===================================================================\n                         Create a dataframe \n        ====================================================================\"\"\"        \n        # add scalar indices into the df_indices dataframe\n        df_indices = pd.concat([df_audio_ind,\n                                df_spec_ind], axis=1)\n    \n        # add date and audio_path\n        df_indices.insert(0, 'Date', date)\n        df_indices.insert(1, 'file', audio_path)\n    \n    except:\n        # if an error occur, send an empty output\n        df_indices = pd.DataFrame()\n        \n    return df_indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "\"\"\" ===========================================================================\n                 Mono CPU \n============================================================================\"\"\"\n# Only one cpu is used to process all data in dataframe, row by row.\n# This is the common way to process the data but it has some limitations :\n# - only 1 CPU is used even if the computer has more CPUs.\n# - data are sequentially processed which means each file will wait for the \n# completion of the previous file in the list. If 1 file requires more time to\n# be processed, the time to complete the overall process will take longer.\n\n# create an empty dataframe. It will contain all ROIs found for each\n# audio file in the directory\ndf_indices = pd.DataFrame()     \n  \ntic = time.perf_counter()\nwith tqdm(total=len(df), desc=\"unique cpu indices calculation...\") as pbar:          \n    for index, row in df.iterrows() :\n        df_indices_temp = single_file_processing(row[\"file\"], row[\"Date\"])\n        pbar.update(1)\n        df_indices = df_indices.append(df_indices_temp)\ntoc = time.perf_counter()\n\n# time duration of the process\nmonocpu_duration = toc - tic\n\nprint(f\"Elapsed time is {monocpu_duration:0.1f} seconds\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "\"\"\" ===========================================================================\n                 Multi CPU\n============================================================================\"\"\"\n# At least 2 CPUs will be used in parallel and the files to process will be \n# distributed on each CPU depending on their availability. This will speed up\n# the process.\n\n# create an empty dataframe. It will contain all ROIs found for each\n# audio file in the directory\ndf_indices = pd.DataFrame()\n\n# Number of CPU used for the calculation. \nnb_cpu = os.cpu_count()\n\ntic = time.perf_counter()\n# Multicpu process\nwith tqdm(total=len(df), desc=\"multi cpu indices calculation...\") as pbar:\n    with futures.ProcessPoolExecutor(max_workers=nb_cpu) as pool:\n        # give the function to map on several CPUs as well its arguments as \n        # as list\n        for df_indices_temp in pool.map(\n            single_file_processing, \n            df[\"file\"].to_list(), \n            df[\"Date\"].to_list()\n        ):\n            pbar.update(1)\n            df_indices = df_indices.append(df_indices_temp)\ntoc = time.perf_counter()\n\n# time duration of the process\nmulticpu_duration = toc - tic\n\nprint(f\"Elapsed time is {multicpu_duration:0.1f} seconds\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Display the comparison between to methods\n-----------------------------------------\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plt.style.use('ggplot')\nfig, ax = plt.subplots(1,1,figsize=(6,2))\n\n# bar graphs\nwidth = 0.75\nx = ['sequential (mono CPU)', 'parallel (multi CPU)']\ny = [monocpu_duration, multicpu_duration]\nax.barh(x, y, width, color=('tab:blue','tab:orange'))\nax.set_xlabel('Elapsed time (s)')\nax.set_title(\"Comparison between sequential\\n and parallel processing\")\n\nfig.tight_layout()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}