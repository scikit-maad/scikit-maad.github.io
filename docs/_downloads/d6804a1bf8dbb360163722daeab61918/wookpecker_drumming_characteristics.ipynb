{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\nDownload audio files from Xeno-Canto and automatically extract characteristics\n===============================================================================\n \nThe goal of this example is to show how to automatically download audio files\nfrom Xeno-Canto and process them to automatically extract audio characteristics\nin order to classify the sound made by different species.\nWe focus on the characteristics of the drumming performed by the woodpeckers \nspecies that are present in Europe.\n\n**Dependencies**: To execute this example you need to have installed the \nlibrosa, sklearn and pandas Python packages.\n\n\n(from https://woodpeckersofeurope.blogspot.com/2007/11/drumming.html)\nWoodpeckers of Europe\n10 species of woodpecker (Picidae) breed in Europe: \n9 resident species and the migratory Wryneck. 8 of these 10 also occur \noutside Europe, with the distribution of Eurasian Three-toed, White-backed, \nLesser Spotted, Great Spotted, Black & Grey-headed Woodpeckers stretching \neastwards from the Western Palearctic into Asia, whilst Syrian is found in \nthe Middle East & Asia Minor & Wryneck winters in Africa. The global ranges \nof Green & Middle Spotted Woodpeckers are confined to the Western Palearctic.\n\nEurasian Three-toed : Picoides tridactylus\nWhite-backed : Dendrocopos leucotos\nLesser Spotted :        Dryobates minor\nGreat Spotted : Dendrocopos major\nBlack :         Dryocopus martius\nGrey-headed :   Picus canus\nSyrian : Dendrocopos syriacus\nWryneck : Jynx torquilla\nGreen : Picus viridis\nMiddle Spotted : Dendrocoptes medius\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# sphinx_gallery_thumbnail_path = './_images/sphx_glr_plot_woodpeckers_drumming_clusters.png'\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib.lines import Line2D \nimport numpy as np\nfrom pathlib import Path  \nimport sys\nimport os\nimport time \nimport warnings\n# suppress all warnings\nwarnings.filterwarnings(\"ignore\")\n\nfrom scipy import signal\nimport librosa\nimport librosa.display\nfrom sklearn.manifold import TSNE\nfrom sklearn.preprocessing import StandardScaler\nfrom maad import sound, util, rois"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define local function\n---------------------\nfunction to grab all audio files in a folder\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def grab_audio(path, audio_format='mp3'):\n    filelist = []\n    for root, dirs, files in os.walk(path, topdown=False):\n        for name in files:\n            if name[-3:].casefold() == audio_format and name[:2] != '._':\n                filelist.append(os.path.join(root, name))\n    return filelist"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define constants\n----------------\nDirectory where to download the audiofile from xeno-canto\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "XC_ROOTDIR = '../../data/'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Name of the dataset. This will be used to create a subdir with the same name \n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "XC_DIR = 'woodpecker_dataset' \n\ndata = [['Eurasian Three-toed', 'Picoides tridactylus'],\n        ['White-backed',        'Dendrocopos leucotos'],\n        ['Lesser Spotted',     \t'Dryobates minor'],\n        ['Great Spotted',       'Dendrocopos major'],\n        ['Black',              \t'Dryocopus martius'],\n        ['Grey-headed',        \t'Picus canus'],\n        ['Syrian',              'Dendrocopos syriacus'],\n        ['Wryneck',             'Jynx torquilla'],\n        ['Green',               'Picus viridis'],\n        ['Middle Spotted',      'Dendrocoptes medius']]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Query Xeno-Canto\n----------------\nget the genus and species needed for Xeno-Canto\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "df_species = pd.DataFrame(data,columns =['english name',\n                                         'scientific name'])\ngen = []\nsp = []\nfor name in df_species['scientific name']:\n    gen.append(name.rpartition(' ')[0])\n    sp.append(name.rpartition(' ')[2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Build the query dataframe with columns paramXXX\ngen : genus\ncnt : country\narea : continent (europe, america, asia, africa)\nq : quality \nlen : length of the audio file \ntype : type of sound : 'song' or 'call' or 'drumming'\nPlease have a look here to know all the parameters and how to use them :\nhttps://xeno-canto.org/help/search\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "df_query = pd.DataFrame()\ndf_query['param1'] = gen\ndf_query['param2'] = sp\ndf_query['param3'] ='type:drumming'\ndf_query['param4'] ='area:europe'\n# df_query['param5 ='len:\"5-120\"'\n# df_query['param6'] ='q:\">C\"'\n\n# Get recordings metadata corresponding to the query\ndf_dataset= util.xc_multi_query(df_query, \n                                 format_time = False,\n                                 format_date = False,\n                                 verbose=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Download audio Xeno-Canto\n--------------------------\nFrom the metadata that was collected in the previous section, \nselect a maximum of 20 files per species, regarding the quality and the length\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "df_dataset = util.xc_selection(df_dataset,\n                               max_nb_files=20,\n                               max_length='01:00',\n                               min_length='00:10',\n                               min_quality='B',\n                               verbose = True )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "download all the audio files into a directory with a subdirectory for each \nspecies\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "util.xc_download(df_dataset,\n                 rootdir = XC_ROOTDIR,\n                 dataset_name= XC_DIR,\n                 overwrite=False,\n                 save_csv= True,\n                 verbose = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Grab all audio filenames in the directory\n------------------------------------------\n create a dataframe with all recordings in the directory\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "filelist = grab_audio(XC_ROOTDIR+XC_DIR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create new columns with short filename and species names\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "df = pd.DataFrame()\nfor file in filelist:\n    df = df.append({'fullfilename': file,\n                    'filename': Path(file).parts[-1][:-4],\n                    'species': Path(file).parts[-2]},\n                   ignore_index=True)\n\nprint('=====================================================')\nprint('number of files : %2.0f' % len(df))\nprint('number of species : %2.0f' % len(df.species.unique()))\nprint('=====================================================')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Process all audio files, species by species\n--------------------------------------------\n In this part, all audio file will be processed in order to extract each\n drumming portion separately. \n Then pulses are automaticaly detected for each drumming before computing\n drumming parameters such as median pulse rate, duration, number of pulses...\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# store starting time \nstart_time = time.time() \n\n# Create an empty dataframe to drummings parameters\ndf_drums = pd.DataFrame()\n\n# Loop to extract portion of the dataframe corresponding to a single species\nfor species in df.species.unique():\n    # get the dataframe corresponding to the current species\n    current_df = df[df.species == species]\n    # Display the current species\n    print ('\\n')\n    print (' %s ' %species)\n    \n    # Loop to load and process each audio file of the current species\n    fullfilename_list = list(current_df.fullfilename)\n    idx = 0\n    for fullfilename in fullfilename_list:    \n        # Create a temporary dataframe in order to store the parameters of \n        # the drummings found in the current audio file\n        df_drums_temp = pd.DataFrame()\n                \n        # extract audio filename and species\n        path, filename_with_ext = os.path.split(fullfilename)\n        _, species = os.path.split(path)\n        file = os.path.splitext(filename_with_ext)[0]\n        \n        # 1. load audio\n        s, fs = librosa.load(fullfilename, sr=16000)\n        \n        # 2. save parameters in dataframe for each file\n        df.loc[df.fullfilename == fullfilename, 'length'] = len(s)\n        df.loc[df.fullfilename == fullfilename, 'sampling_freq'] = fs \n    \n        # 3. bandpass filter around drumming frequencies \n        fcut = (25, 5000)\n        s = sound.select_bandwidth(s,\n                                   fs,\n                                   fcut=fcut,\n                                   forder=1,\n                                   fname='butter',\n                                   ftype='bandpass')\n        \n        # 4. find portions of the signal (ROIs) that contain a drumming\n        df_rois = rois.find_rois_cwt(s, \n                                     fs, \n                                     flims=(50,4000), \n                                     tlen=4, \n                                     th=1e-3, \n                                     display=False)\n        \n        # 5. Loop to process each ROI previously found \n        pulseRateMedian = []\n        drum_duration = []\n        n_pulses = []\n        interval_min = []\n        interval_max = []\n        interv1_intervMax = []\n        intervL_intervMax = []\n        MeanAcc = []\n        Amp_pulses_min = []\n        AMp_pulses_first = []\n        Amp_pulses_last = []\n                \n        # Loop\n        for index, row in df_rois.iterrows():\n            # trim sound to process only portion of the sound that correspond\n            # to the current ROI\n            s_trim = sound.trim(s, fs, row['min_t'], row['max_t'])\n            s_trim = s_trim - np.mean(s_trim)\n            s_trim = s_trim / np.max(np.abs(s_trim))\n            \n            # compute fast enveloppe with windows of 32 samples\n            env = sound.envelope(s_trim, Nt=32)\n            \n            if np.median(env) < 0.1 :\n                pulses, pulses_info = signal.find_peaks(env, distance=15, height = np.median(env)*2, prominence=0.2) \n                                \n                # convert pulses in sample into seconds\n                pulses = pulses/fs*32 \n                # get the relative pulse amplitude\n                pulse_heights = pulses_info['peak_heights']\n                \n                if (len(pulses) > 10) and (np.max(np.diff(pulses))<0.2) and (1/np.median(np.diff(pulses))>10):\n                    pulseRateMedian += [1/np.median(np.diff(pulses))]\n                    drum_duration += [pulses[-1] - pulses[0]]\n                    n_pulses += [len(pulses)]\n                    interval_min += [np.min(np.diff(pulses))]\n                    interval_max += [np.max(np.diff(pulses))]\n                    interv1_intervMax += [pulses[1] - pulses[0]]\n                    intervL_intervMax += [pulses[-2] - pulses[-1]]\n                    MeanAcc += [np.mean(np.diff(np.diff(pulses)))]\n                    Amp_pulses_min += [np.min(pulse_heights)]\n                    AMp_pulses_first += [pulse_heights[0]]\n                    Amp_pulses_last += [pulse_heights[-1]]\n                    \n                    # plot some envelopes with peak detection\n                    # if idx%10 == 0 :\n                    #     plt.figure()\n                    #     plt.plot(env)\n                    #     plt.plot(pulses*fs/32, env[(pulses*fs/32).astype('int')], \"x\")\n                    #     plt.show()\n                    \n                else :\n                    pulseRateMedian += [np.nan]\n                    drum_duration += [np.nan]\n                    n_pulses += [np.nan]\n                    interval_min += [np.nan]\n                    interval_max += [np.nan]\n                    interv1_intervMax += [np.nan]\n                    intervL_intervMax += [np.nan]\n                    MeanAcc += [np.nan]\n                    Amp_pulses_min += [np.nan]\n                    AMp_pulses_first += [np.nan]\n                    Amp_pulses_last += [np.nan]\n            \n        if len(df_drums) == 0 :\n            df_drums['pulseRateMedian'] = pulseRateMedian \n            df_drums['drum_duration'] = drum_duration \n            df_drums['n_pulses'] = n_pulses \n            df_drums['interval_min'] = interval_min \n            df_drums['interval_max'] = interval_max \n            df_drums['interv1_intervMax'] = interv1_intervMax \n            df_drums['intervL_intervMax'] = intervL_intervMax \n            df_drums['MeanAcc'] = MeanAcc \n            df_drums['Amp_pulses_min'] = Amp_pulses_min \n            df_drums['AMp_pulses_first'] = AMp_pulses_first \n            df_drums['Amp_pulses_last'] = Amp_pulses_last \n            df_drums['species'] = species \n            df_drums['filename'] = file   \n        else:\n            df_drums_temp['pulseRateMedian'] = pulseRateMedian \n            df_drums_temp['drum_duration'] = drum_duration \n            df_drums_temp['n_pulses'] = n_pulses \n            df_drums_temp['interval_min'] = interval_min \n            df_drums_temp['interval_max'] = interval_max \n            df_drums_temp['interv1_intervMax'] = interv1_intervMax \n            df_drums_temp['intervL_intervMax'] = intervL_intervMax \n            df_drums_temp['MeanAcc'] = MeanAcc \n            df_drums_temp['Amp_pulses_min'] = Amp_pulses_min \n            df_drums_temp['AMp_pulses_first'] = AMp_pulses_first \n            df_drums_temp['Amp_pulses_last'] = Amp_pulses_last\n            df_drums_temp['species'] = species \n            df_drums_temp['filename'] = file \n            df_drums = df_drums.append(df_drums_temp, ignore_index=True)                   \n      \n        # counter\n        sys.stdout.write('\\r')\n        sys.stdout.write('%2.0f%%' %np.round(((idx+1)/len(current_df)*100))) \n        idx = idx+1\n                \nprint(\"--- %2.2f minutes ---\" % ((time.time() - start_time)/60))\n\n# drop all rows with NaN\ndf_drums = df_drums.dropna()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Display boxplot\n----------------\n Display a boxplot of the feature \"pulseRateMedian\" for each species\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plt.style.use('ggplot')\n\n# create a figure\nfig = plt.figure(figsize= (7,3))\nax = fig.add_subplot(111)\nn = 0\n# loop to build a boxplot for each species based on the feature \"pulseRateMedian\"\nfor species in df_drums.species.unique():\n    ax.boxplot(df_drums[df_drums.species == species]['pulseRateMedian'], \n               positions=[n+1],\n               widths = 0.75,\n               vert = False,\n               notch=True)\n    n += 1\nax.set_yticks(np.arange(1,len(df_drums.species.unique())+1))\nax.set_yticklabels(df_drums.species.unique(),\n                   fontsize=9)\nax.set_xlabel('pulseRateMedian [Hz]')\nax.set_ylabel('species')\nax.set_xlim(0,30)\nplt.tight_layout()\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Display clusters based on the drummings features\n-------------------------------------------------\n A collection of features is associated to each drumming found in the audio\n recordings. The goal is to display clusters in 2D with the dimensionality\n reduction tool t-SNE and associate a color to each point that corresponds\n to the belonging species. It is then possible to observe species that are\n clearly grouped into separate clusters from species that are mixed with \n others \n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Preprocess data : data scaler\ndf_drums = df_drums.dropna()\nX = df_drums.drop(columns=['species','filename'])\nscaler = StandardScaler()\nX = scaler.fit_transform(X)\n\n# compute the dimensionality reduction\ntsne = TSNE(n_components=2, \n            perplexity=30, \n            init='pca', \n            n_iter = 5000,\n            n_jobs = -1,\n            verbose=True)\nY = tsne.fit_transform(X)\n\n# overlay all species\nplt.figure(figsize=(5,6))\ng = []\nmarkers = Line2D.filled_markers\nfor species in df_drums.species.unique():\n    g.append(plt.scatter(Y[(df_drums['species'] == species), 0],\n                         Y[(df_drums['species'] == species), 1],\n                         marker = markers[np.random.randint(0, len(markers))],\n                         alpha=0.75))\n\nplt.legend(g, \n           df_drums.species.unique(), \n           bbox_to_anchor=(0, -0.1), \n           loc='upper left', \n           fontsize=8,\n           frameon = True)\nplt.tight_layout()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}